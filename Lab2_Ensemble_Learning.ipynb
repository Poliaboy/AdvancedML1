{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab: Regression Task Using Random Forest, XGBoost, and LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Objective**:\n",
    "In this lab, you will learn how to apply three powerful ensemble learning algorithms—**Random Forest**, **XGBoost**, and **LightGBM**—to solve a regression problem. You will explore how to train and evaluate these models on a sample dataset, understand their strengths, and compare their performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Prerequisites**:\n",
    "- Familiarity with Python and common ML libraries (`pandas`, `scikit-learn`).\n",
    "- Basic understanding of regression metrics such as Mean Squared Error (MSE) and R-squared.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Libraries to Install**:\n",
    "Make sure you have the following libraries installed before running the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Install the required libraries\n",
    "!pip install pandas scikit-learn xgboost lightgbm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Importing Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Dataset: California Housing Prices**\n",
    "\n",
    "For this lab, we will use the **California Housing Prices** dataset, which is available from the `scikit-learn` dataset module. This dataset contains features like average income, house age, and house prices in various districts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 2.1: Load the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  \n",
       "0    -122.23  \n",
       "1    -122.22  \n",
       "2    -122.24  \n",
       "3    -122.25  \n",
       "4    -122.25  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load the dataset\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name=\"MedHouseVal\")\n",
    "\n",
    "# Display the first few rows\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 2.2: Split the Data**\n",
    "\n",
    "We will split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (16512, 8), Testing data: (4128, 8)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training data: {X_train.shape}, Testing data: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3. Model 0: Decision tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree MSE: 0.4997\n",
      "Decision Tree R²: 0.6187\n",
      "Decision Tree Training Time: 0.1162 seconds\n",
      "Decision Tree Training MSE: 0.0000\n",
      "Decision Tree Training R²: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dt_model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "start_time = time.time()\n",
    "dt_model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "training_time_dt = end_time - start_time\n",
    "\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
    "r2_dt = r2_score(y_test, y_pred_dt)\n",
    "\n",
    "mse_dt_train = mean_squared_error(y_train, dt_model.predict(X_train))\n",
    "r2_dt_train = r2_score(y_train, dt_model.predict(X_train))\n",
    "\n",
    "print(f\"Decision Tree MSE: {mse_dt:.4f}\")\n",
    "print(f\"Decision Tree R²: {r2_dt:.4f}\")\n",
    "print(f\"Decision Tree Training Time: {training_time_dt:.4f} seconds\")\n",
    "print(f\"Decision Tree Training MSE: {mse_dt_train:.4f}\")\n",
    "print(f\"Decision Tree Training R²: {r2_dt_train:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **3. Model 1: Random Forest Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 3.1: Train the Random Forest Regressor**\n",
    "\n",
    "We’ll start with the **Random Forest Regressor**, which is an ensemble learning method that builds multiple decision trees and averages their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestRegressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "rf_model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "training_time_rf = end_time - start_time\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_rf = rf_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 3.2: Evaluate the Random Forest Regressor**\n",
    "\n",
    "We will evaluate the performance of the Random Forest model using **Mean Squared Error (MSE)** and **R-squared (R²)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest MSE: 0.2557\n",
      "Random Forest R²: 0.8049\n",
      "Random Forest Training Time: 6.3041 seconds\n",
      "Random Forest Training MSE: 0.0354\n",
      "Random Forest Training R²: 0.9735\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "mse_rf_train = mean_squared_error(y_train, rf_model.predict(X_train))\n",
    "r2_rf_train = r2_score(y_train, rf_model.predict(X_train))\n",
    "\n",
    "print(f\"Random Forest MSE: {mse_rf:.4f}\")\n",
    "print(f\"Random Forest R²: {r2_rf:.4f}\")\n",
    "print(f\"Random Forest Training Time: {training_time_rf:.4f} seconds\")\n",
    "\n",
    "print(f\"Random Forest Training MSE: {mse_rf_train:.4f}\")\n",
    "print(f\"Random Forest Training R²: {r2_rf_train:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Model 2: XGBoost Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 4.1: Train the XGBoost Regressor**\n",
    "\n",
    "Next, we will train the **XGBoost** model, which uses gradient boosting techniques to optimize decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the XGBoost regressor\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "training_time_xgb = end_time - start_time\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_xgb = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 4.2: Evaluate the XGBoost Regressor**\n",
    "We will now evaluate the performance of the XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost MSE: 0.2273\n",
      "XGBoost R²: 0.8266\n",
      "XGBoost Training Time: 0.1864 seconds\n",
      "XGBoost Training MSE: 0.1361\n",
      "XGBoost Training R²: 0.8982\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "mse_xgb_train = mean_squared_error(y_train, xgb_model.predict(X_train))\n",
    "r2_xgb_train = r2_score(y_train, xgb_model.predict(X_train))\n",
    "\n",
    "print(f\"XGBoost MSE: {mse_xgb:.4f}\")\n",
    "print(f\"XGBoost R²: {r2_xgb:.4f}\")\n",
    "print(f\"XGBoost Training Time: {training_time_xgb:.4f} seconds\")\n",
    "print(f\"XGBoost Training MSE: {mse_xgb_train:.4f}\")\n",
    "print(f\"XGBoost Training R²: {r2_xgb_train:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Model 3: LightGBM Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 5.1: Train the LightGBM Regressor**\n",
    "\n",
    "Now, we will use **LightGBM**, another gradient boosting algorithm known for its speed and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000500 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1838\n",
      "[LightGBM] [Info] Number of data points in the train set: 16512, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.071947\n"
     ]
    }
   ],
   "source": [
    "# Initialize the LightGBM regressor\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "lgb_model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "training_time_lgb = end_time - start_time\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_lgb = lgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 5.2: Evaluate the LightGBM Regressor**\n",
    "\n",
    "Finally, evaluate the performance of the LightGBM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM MSE: 0.2148\n",
      "LightGBM R²: 0.8360\n",
      "LightGBM Training Time: 0.4720 seconds\n",
      "LightGBM Training MSE: 0.1562\n",
      "LightGBM Training R²: 0.8831\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "mse_lgb = mean_squared_error(y_test, y_pred_lgb)\n",
    "r2_lgb = r2_score(y_test, y_pred_lgb)\n",
    "\n",
    "mse_lgb_train = mean_squared_error(y_train, lgb_model.predict(X_train))\n",
    "r2_lgb_train = r2_score(y_train, lgb_model.predict(X_train))\n",
    "\n",
    "print(f\"LightGBM MSE: {mse_lgb:.4f}\")\n",
    "print(f\"LightGBM R²: {r2_lgb:.4f}\")\n",
    "print(f\"LightGBM Training Time: {training_time_lgb:.4f} seconds\")\n",
    "print(f\"LightGBM Training MSE: {mse_lgb_train:.4f}\")\n",
    "print(f\"LightGBM Training R²: {r2_lgb_train:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Comparing the Models**\n",
    "We will now compare the performance of the three models using **MSE** and **R²**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Comparison:\n",
      "Random Forest MSE: 0.2557, R²: 0.8049\n",
      "XGBoost MSE: 0.2273, R²: 0.8266\n",
      "LightGBM MSE: 0.2148, R²: 0.8360\n"
     ]
    }
   ],
   "source": [
    "# Print comparison of the three models\n",
    "print(\"Model Comparison:\")\n",
    "print(f\"Random Forest MSE: {mse_rf:.4f}, R²: {r2_rf:.4f}\")\n",
    "print(f\"XGBoost MSE: {mse_xgb:.4f}, R²: {r2_xgb:.4f}\")\n",
    "print(f\"LightGBM MSE: {mse_lgb:.4f}, R²: {r2_lgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time comparison:\n",
      "Random Forest Training Time: 6.3041 seconds\n",
      "XGBoost Training Time: 0.1864 seconds\n",
      "LightGBM Training Time: 0.4720 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Training time comparison:\")\n",
    "print(f\"Random Forest Training Time: {training_time_rf:.4f} seconds\")\n",
    "print(f\"XGBoost Training Time: {training_time_xgb:.4f} seconds\")\n",
    "print(f\"LightGBM Training Time: {training_time_lgb:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7. Hyperparameter Tuning (if time is left)**\n",
    "\n",
    "For advanced users, you can improve model performance by tuning hyperparameters. Here’s an example of how to use GridSearchCV to tune hyperparameters for **Random Forest**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=50; total time=   3.8s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=50; total time=   3.8s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=50; total time=   3.6s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=50; total time=   3.9s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=50; total time=   3.7s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=50; total time=   3.7s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   6.7s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   7.0s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time=   7.0s\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=50; total time=   3.1s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   6.1s\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=50; total time=   2.9s\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=50; total time=   3.2s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   6.6s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=100; total time=   6.2s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=  13.6s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=  13.7s\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=200; total time=  13.9s\n",
      "[CV] END .max_depth=10, min_samples_split=2, n_estimators=50; total time=   1.9s\n",
      "[CV] END .max_depth=10, min_samples_split=2, n_estimators=50; total time=   2.0s\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   5.6s\n",
      "[CV] END .max_depth=10, min_samples_split=2, n_estimators=50; total time=   2.1s\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=  12.2s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=  12.1s\n",
      "[CV] END max_depth=None, min_samples_split=5, n_estimators=200; total time=  12.0s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   4.1s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   4.3s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=100; total time=   4.3s\n",
      "[CV] END .max_depth=10, min_samples_split=5, n_estimators=50; total time=   2.2s\n",
      "[CV] END .max_depth=10, min_samples_split=5, n_estimators=50; total time=   2.1s\n",
      "[CV] END .max_depth=10, min_samples_split=5, n_estimators=50; total time=   2.2s\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=  11.2s\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=  10.8s\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=200; total time=  10.6s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=50; total time=   2.0s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=50; total time=   2.0s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   8.0s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   8.0s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=50; total time=   1.9s\n",
      "[CV] END max_depth=10, min_samples_split=2, n_estimators=200; total time=   7.9s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   3.5s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   7.4s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   7.4s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   3.7s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=100; total time=   3.9s\n",
      "[CV] END max_depth=10, min_samples_split=5, n_estimators=200; total time=   7.6s\n",
      "[CV] END .max_depth=20, min_samples_split=2, n_estimators=50; total time=   3.0s\n",
      "[CV] END .max_depth=20, min_samples_split=2, n_estimators=50; total time=   2.9s\n",
      "[CV] END .max_depth=20, min_samples_split=2, n_estimators=50; total time=   2.8s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   7.7s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   7.7s\n",
      "[CV] END .max_depth=20, min_samples_split=5, n_estimators=50; total time=   2.8s\n",
      "[CV] END max_depth=10, min_samples_split=10, n_estimators=200; total time=   7.7s\n",
      "[CV] END .max_depth=20, min_samples_split=5, n_estimators=50; total time=   2.7s\n",
      "[CV] END .max_depth=20, min_samples_split=5, n_estimators=50; total time=   2.8s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   5.3s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=50; total time=   2.7s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=50; total time=   2.6s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=50; total time=   2.7s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=  11.6s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=  11.5s\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=200; total time=  11.6s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   5.5s\n",
      "[CV] END .max_depth=30, min_samples_split=2, n_estimators=50; total time=   3.1s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   5.4s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time=   5.5s\n",
      "[CV] END .max_depth=30, min_samples_split=2, n_estimators=50; total time=   3.2s\n",
      "[CV] END .max_depth=30, min_samples_split=2, n_estimators=50; total time=   3.2s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=  11.5s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=  11.5s\n",
      "[CV] END max_depth=20, min_samples_split=5, n_estimators=200; total time=  11.5s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=100; total time=   6.1s\n",
      "[CV] END .max_depth=30, min_samples_split=5, n_estimators=50; total time=   2.9s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=  11.0s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=  10.8s\n",
      "[CV] END .max_depth=30, min_samples_split=5, n_estimators=50; total time=   2.8s\n",
      "[CV] END .max_depth=30, min_samples_split=5, n_estimators=50; total time=   2.8s\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=200; total time=  10.9s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=100; total time=   6.0s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=100; total time=   6.0s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=100; total time=   5.8s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=50; total time=   2.9s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=50; total time=   2.8s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=100; total time=   5.7s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=50; total time=   2.9s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=100; total time=   5.9s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=200; total time=  12.1s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=200; total time=  12.5s\n",
      "[CV] END max_depth=30, min_samples_split=2, n_estimators=200; total time=  12.3s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=100; total time=   4.9s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=100; total time=   4.8s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=200; total time=  10.9s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=100; total time=   5.0s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=200; total time=  11.4s\n",
      "[CV] END max_depth=30, min_samples_split=5, n_estimators=200; total time=  10.8s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=200; total time=   8.5s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=200; total time=   8.7s\n",
      "[CV] END max_depth=30, min_samples_split=10, n_estimators=200; total time=   8.6s\n",
      "Best parameters found:  {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters found: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8. Individual Work**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercises**:\n",
    "1. Experiment with the hyperparameters for **XGBoost** and **LightGBM**. Use `GridSearchCV` or `RandomizedSearchCV` to find optimal configurations.\n",
    "2. Try running the models on a different regression dataset (e.g., Boston Housing or any dataset of your choice).\n",
    "3. Analyze the training time of each model using the `time` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=100; total time=   0.2s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=100; total time=   0.3s\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=100; total time=   0.3s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=100; total time=   0.2s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=100; total time=   0.2s\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=200; total time=   0.4s\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=200; total time=   0.4s\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=200; total time=   0.5s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=200; total time=   0.3s\n",
      "[CV] END ................learning_rate=0.2, n_estimators=100; total time=   0.2s\n",
      "[CV] END ................learning_rate=0.2, n_estimators=100; total time=   0.2s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=200; total time=   0.4s\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=300; total time=   0.7s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=200; total time=   0.5s\n",
      "[CV] END ................learning_rate=0.2, n_estimators=100; total time=   0.3s\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=300; total time=   0.7s\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=300; total time=   0.7s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=300; total time=   0.6s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=300; total time=   0.6s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=300; total time=   0.6s\n",
      "[CV] END ................learning_rate=0.2, n_estimators=200; total time=   0.3s\n",
      "[CV] END ................learning_rate=0.2, n_estimators=200; total time=   0.3s\n",
      "[CV] END ................learning_rate=0.2, n_estimators=200; total time=   0.3s\n",
      "[CV] END ................learning_rate=0.2, n_estimators=300; total time=   0.4s\n",
      "[CV] END ................learning_rate=0.2, n_estimators=300; total time=   0.5s\n",
      "[CV] END ................learning_rate=0.2, n_estimators=300; total time=   0.5s\n",
      "Best parameters found:  {'learning_rate': 0.1, 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "print(\"XGBoost\")\n",
    "params_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb_model, param_grid=params_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters found: \", grid_search_xgb.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001112 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1837\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001150 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1837\n",
      "[LightGBM] [Info] Number of data points in the train set: 11008, number of used features: 8\n",
      "[LightGBM] [Info] Number of data points in the train set: 11008, number of used features: 8\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001750 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1837\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001627 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1837\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001231 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Number of data points in the train set: 11008, number of used features: 8\n",
      "[LightGBM] [Info] Total Bins 1838\n",
      "[LightGBM] [Info] Number of data points in the train set: 11008, number of used features: 8[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001946 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "\n",
      "[LightGBM] [Info] Start training from score 2.064393\n",
      "[LightGBM] [Info] Start training from score 2.064393\n",
      "[LightGBM] [Info] Total Bins 1837\n",
      "[LightGBM] [Info] Number of data points in the train set: 11008, number of used features: 8\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001466 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1838\n",
      "[LightGBM] [Info] Start training from score 2.078156\n",
      "[LightGBM] [Info] Number of data points in the train set: 11008, number of used features: 8\n",
      "[LightGBM] [Info] Number of data points in the train set: 11008, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.073292\n",
      "[LightGBM] [Info] Start training from score 2.078156\n",
      "[LightGBM] [Info] Start training from score 2.064393[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001767 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1837\n",
      "\n",
      "[LightGBM] [Info] Number of data points in the train set: 11008, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.073292[LightGBM] [Info] Start training from score 2.078156\n",
      "\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000668 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1838\n",
      "[LightGBM] [Info] Number of data points in the train set: 11008, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.073292\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000541 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1837\n",
      "[LightGBM] [Info] Number of data points in the train set: 11008, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.064393\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000537 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1837\n",
      "[LightGBM] [Info] Number of data points in the train set: 11008, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.078156\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000523 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1838\n",
      "[LightGBM] [Info] Number of data points in the train set: 11008, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.073292\n",
      "[CV] END ................learning_rate=0.1, n_estimators=100; total time=   4.5s\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=100; total time=   4.8s\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=100; total time=   4.8s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=100; total time=   4.6s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=100; total time=   4.7s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002813 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1837\n",
      "[LightGBM] [Info] Number of data points in the train set: 11008, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.064393\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000953 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1837\n",
      "[LightGBM] [Info] Number of data points in the train set: 11008, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.078156\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=100; total time=   4.9s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036358 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1838\n",
      "[LightGBM] [Info] Number of data points in the train set: 11008, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.073292\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000791 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1837\n",
      "[LightGBM] [Info] Number of data points in the train set: 11008, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.064393\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000547 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1837\n",
      "[LightGBM] [Info] Number of data points in the train set: 11008, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.078156\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000985 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1838\n",
      "[LightGBM] [Info] Number of data points in the train set: 11008, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.073292\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=200; total time=   9.2s\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001312 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1837\n",
      "[LightGBM] [Info] Number of data points in the train set: 11008, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.064393\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=200; total time=   9.2s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000606 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1837\n",
      "[LightGBM] [Info] Number of data points in the train set: 11008, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.078156\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=200; total time=   9.4s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000444 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1838\n",
      "[LightGBM] [Info] Number of data points in the train set: 11008, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.073292\n",
      "[CV] END ................learning_rate=0.1, n_estimators=200; total time=   8.2s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010300 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1837\n",
      "[LightGBM] [Info] Number of data points in the train set: 11008, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.064393\n",
      "[CV] END ................learning_rate=0.1, n_estimators=200; total time=   8.5s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000669 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1837\n",
      "[LightGBM] [Info] Number of data points in the train set: 11008, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.078156\n",
      "[CV] END ................learning_rate=0.2, n_estimators=100; total time=   4.2s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000663 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1838\n",
      "[LightGBM] [Info] Number of data points in the train set: 11008, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.073292\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=300; total time=  13.7s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=200; total time=   8.8s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000615 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1837\n",
      "[LightGBM] [Info] Number of data points in the train set: 11008, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.064393\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=300; total time=  13.7s\n",
      "[CV] END ................learning_rate=0.2, n_estimators=100; total time=   4.1s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000573 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1838\n",
      "[LightGBM] [Info] Number of data points in the train set: 11008, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.073292\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000439 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1837\n",
      "[LightGBM] [Info] Number of data points in the train set: 11008, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.078156\n",
      "[CV] END ...............learning_rate=0.01, n_estimators=300; total time=  14.0s\n",
      "[CV] END ................learning_rate=0.2, n_estimators=100; total time=   5.1s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=300; total time=  11.5s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=300; total time=  11.8s\n",
      "[CV] END ................learning_rate=0.1, n_estimators=300; total time=  11.8s\n",
      "[CV] END ................learning_rate=0.2, n_estimators=200; total time=   5.4s\n",
      "[CV] END ................learning_rate=0.2, n_estimators=200; total time=   5.2s\n",
      "[CV] END ................learning_rate=0.2, n_estimators=200; total time=   5.1s\n",
      "[CV] END ................learning_rate=0.2, n_estimators=300; total time=   5.7s\n",
      "[CV] END ................learning_rate=0.2, n_estimators=300; total time=   6.0s\n",
      "[CV] END ................learning_rate=0.2, n_estimators=300; total time=   6.0s\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000398 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1838\n",
      "[LightGBM] [Info] Number of data points in the train set: 16512, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.071947\n",
      "Best parameters found:  {'learning_rate': 0.1, 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "print(\"LightGBM\")\n",
    "params_grid = {\n",
    "    'n_estimators': [100, 200, 300 ],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_lgb = GridSearchCV(estimator=lgb_model, param_grid=params_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model\n",
    "grid_search_lgb.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters found: \", grid_search_lgb.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:\n",
      "Training Time: 6.3003 seconds\n",
      "MSE: 0.2557\n",
      "R²: 0.8049\n",
      "\n",
      "XGBoost:\n",
      "Training Time: 0.4447 seconds\n",
      "MSE: 0.2085\n",
      "R²: 0.8409\n",
      "\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000273 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1838\n",
      "[LightGBM] [Info] Number of data points in the train set: 16512, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.071947\n",
      "LightGBM:\n",
      "Training Time: 1.1075 seconds\n",
      "MSE: 0.1939\n",
      "R²: 0.8520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the Boston Housing dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import time\n",
    "\n",
    "# Load the dataset\n",
    "california = fetch_california_housing()\n",
    "X = pd.DataFrame(california.data, columns=california.feature_names)\n",
    "y = pd.Series(california.target, name=\"MedHouseVal\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the models\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=300, learning_rate=0.1, random_state=42)\n",
    "lgb_model = lgb.LGBMRegressor(n_estimators=300, learning_rate=0.1, random_state=42)\n",
    "\n",
    "models = [rf_model, xgb_model, lgb_model]\n",
    "model_names = ['Random Forest', 'XGBoost', 'LightGBM']\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    # Train the model and measure time\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"Training Time: {train_time:.4f} seconds\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Research Points**:\n",
    "- How do the three models differ in terms of training time and performance?\n",
    "- Why might LightGBM or XGBoost outperform Random Forest on certain datasets?\n",
    "- How do the ensemble methods like Random Forest and boosting methods like XGBoost and LightGBM handle overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How do the three models differ in terms of training time and performance?\n",
    "\n",
    "Training Time:\n",
    "1. Random Forest: 6.3003 seconds\n",
    "2. XGBoost: 0.4447 seconds\n",
    "3. LightGBM: 1.1075 seconds\n",
    "\n",
    "Performance (MSE and R²):\n",
    "1. Random Forest: MSE: 0.2557 R²: 0.8049\n",
    "2. XGBoost: MSE: 0.2085 R²: 0.8409\n",
    "3. LightGBM: MSE: 0.1939 R²: 0.8520\n",
    "\n",
    "The models differ significantly in training time, with XGBoost being the fastest, followed by LightGBM, and Random Forest taking considerably longer. In terms of performance, LightGBM slightly outperforms XGBoost, which in turn outperforms Random Forest. The differences in R² scores are relatively small, but consistent across the models.\n",
    "\n",
    "2. Why might LightGBM or XGBoost outperform Random Forest on certain datasets?\n",
    "\n",
    "LightGBM and XGBoost might outperform Random Forest for several reasons:\n",
    "\n",
    "- Gradient Boosting: Both LightGBM and XGBoost use gradient boosting, which builds trees sequentially to correct errors from previous trees. This can lead to better performance on complex datasets.\n",
    "- Feature importance: These algorithms have sophisticated methods for determining feature importance, which can lead to better use of relevant features.\n",
    "- Regularization: Both include built-in regularization techniques, which can help prevent overfitting.\n",
    "- Handling of categorical variables: LightGBM, in particular, has efficient methods for handling categorical variables.\n",
    "- Optimization algorithms: They use more advanced optimization algorithms compared to Random Forest.\n",
    "\n",
    "3. How do the ensemble methods like Random Forest and boosting methods like XGBoost and LightGBM handle overfitting?\n",
    "\n",
    "These ensemble methods handle overfitting in different ways:\n",
    "\n",
    "Random Forest:\n",
    "- Uses bagging (bootstrap aggregating) to create diverse trees\n",
    "- Each tree is trained on a random subset of data and features\n",
    "- Aggregates predictions from multiple trees, reducing variance\n",
    "\n",
    "XGBoost and LightGBM:\n",
    "- Use boosting to sequentially improve weak learners\n",
    "- Employ regularization techniques (L1, L2) to penalize complex models\n",
    "- Feature subsampling at each iteration to introduce randomness\n",
    "- Early stopping to prevent overfitting during training\n",
    "- LightGBM uses leaf-wise growth instead of level-wise, which can lead to deeper trees without overfitting\n",
    "\n",
    "All three methods benefit from ensemble learning, which generally helps to reduce overfitting by combining multiple models. However, the boosting methods (XGBoost and LightGBM) often require more careful tuning of hyperparameters to prevent overfitting, while Random Forest is generally more robust out-of-the-box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a hands-on approach to comparing **Random Forest**, **XGBoost**, and **LightGBM** for a regression task. It introduces the core concepts and provides insights into their performance, with opportunities for deeper exploration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
